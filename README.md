# Two-Tower Retrieval Model

This project implements a Two-Tower neural retrieval model for the MS MARCO dataset. Queries and their associated positive passages are encoded separately, and training uses a custom triplet loss with in-batch negatives.

---

## ğŸ—‚ Directory Structure

```
.
â”œâ”€â”€ checkpoints/                 # Saved model checkpoints
â”œâ”€â”€ conda_env.yml               # Environment setup with Mamba
â”œâ”€â”€ data/                       # Parquet files, GloVe embeddings, vocab
â”œâ”€â”€ .env                        # WandB API key (not tracked)
â”œâ”€â”€ misc/                       # Debug and exploration scripts
â”œâ”€â”€ notebooks/                  # Dataset download, token updates, tests
â”œâ”€â”€ scratch/                    # Intermediate dev work (not used in final pipeline)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ config.py               # Global configs for trianing 
â”‚   â”œâ”€â”€ dataloader.py           # Loads queries, positive passages, and samples negatives
â”‚   â””â”€â”€ train.py                # Full training loop, model, and metrics
â””â”€â”€ structure.txt               # Tree structure of the repo
```

---

## ğŸ“¦ Setup

1. **Install with Mamba:**
```bash
mamba env create -f conda_env.yml
mamba activate two_tower_env
```

2. **Set up WandB API key**  
   Create a `.env` file:
```
WANDB_API_KEY=your_key_here
```

---

## ğŸ“‹ Dataset Info

We use the MS MARCO v2.1 dataset, downloaded via Hugging Face:

```python
from datasets import load_dataset
dataset = load_dataset("ms_marco", "v2.1")
```

We save the splits as parquet in `data/`.

| Column                  | Type               | Notes                                                |
| -----------------------|-------------------|------------------------------------------------------|
| `query`                | string            | Raw query                                            |
| `passages.passage_text`| list of strings   | 10 candidate passages                                |
| `passages.is_selected` | list of ints      | Labels (not used; all 10 are treated as positive)   |

---

## ğŸ”¡ Vocab + Embeddings

- Pretrained GloVe embeddings: `glove.6B.200d.txt`
- We added `<pad>` (zero vector) and `<unk>` (mean embedding) tokens to the end.
- Process done in `notebooks/add_tokens.ipynb`.

---

## ğŸ§  Model Overview

Implemented in `scripts/train.py`:

- **Encoders**: Two separate RNNs for queries and passages.
- **Loss**: Custom `full_batch_triplet_loss`
  - Each query is paired with 10 positive passages.
  - Each positive is matched with one negative sampled from other queries.
- **Metrics**: `recall@1`, `recall@5`, and `gap`

---

## ğŸ§ª Dataloader Details

In `scripts/dataloader.py`:

- Each batch contains:
  - `B` queries
  - `B x 10` positive passages
- Negatives are drawn dynamically during training from other examples in the batch.

---

## ğŸš€ Training

Run:

```bash
python scripts/train.py
```

Logs to [Weights & Biases](https://wandb.ai/) if `.env` is set.

Final model is saved to `checkpoints/two_tower_final.pt`.

---

## ğŸ“ Notebooks

- `download_dataset.ipynb`: Fetch MS MARCO from Hugging Face
- `add_tokens.ipynb`: Add `<pad>` and `<unk>` to vocab + embeddings
- `two_tower_test_working_sample.ipynb`: Basic end-to-end test

---

## âœ… Status

- âœ… Full support for multiple positives per query
- âœ… Dynamic in-batch negative sampling
- âœ… Works with GloVe-based initialisation
- âœ… Logging to wandb

---

## ğŸ“š Citation

Dataset:  
> MS MARCO: https://huggingface.co/datasets/microsoft/ms_marco